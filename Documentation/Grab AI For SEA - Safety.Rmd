---
title: "Grab AI For SEA - Safety"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(error = recover)
library(knitr)
library(xtable)
library(data.table)
```

# Grab AI For SEA - Safety

This repository is created for the submission for the Grab AI For SEA Challenge (https://www.aiforsea.com/).

As part of the challenge, I have signed up for the question under the theme of Safety. I have used the R programming language in this challenge.

The scripts used in this project, including the one used for generating this document can be found in the folder "Working Code". This document is generated by the R Markdown file "Working Code/Output/Documentation/Grab AI For SEA.Rmd".

# The Data

The data provided is made up of:
1. Telematics Data
2. Label of Dangerous Driving

The telematics data has a total number of 16135561 rows and 11 columns, split into 10 csv files.

The following is the description as obtained from the website of the competition (www.aiforsea.com/safety)

```{r, results = "asis"}
FieldDesc <-
  data.table(Field = c("bookingID", "Accuracy", "Bearing", "acceleration_x",
                       "acceleration_y", "acceleration_z", "gyro_x", "gyro_y",
                       "gyro_z", "second", "Speed"),
             Description = c("trip id", "accuracy inferred by GPS in meters",
                             "GPS bearing in degree",
                             "accelerometer reading at x axis (m/s2)",
                             "accelerometer reading at y axis (m/s2)",
                             "accelerometer reading at z axis (m/s2)",
                             "gyroscrope reading in x axis (rad/s)",
                             "gyroscope reading in y axis (rad/s)",
                             "gyroscope reading in z axis (rad/s)",
                             "time of the record by number of seconds",
                             "speed measured by GPS in m/s"))

FieldDesc <- kable(FieldDesc)
print(FieldDesc)

```

The label of dangerous driving records in binary form of whether a trip is considered dangerous, 1 if yes and 0 otherwise.

Note that there are some of the bookings with multiple labels. In such instances, I have classified the particular booking as dangerous driving if there is at least ONE label of 1.

# Feature Engineering

Based on the fields in the telematics data, feature engineering has to be conducted in order to obtain aggregate features that are meaningful in determining the riskiness in a particular booking.

## Key Concepts Applied

At the first sight, while it is tempting to just aggregate the numbers in the telematics data as they are, it can be highly unintuitive.

The reason is because the car may be accelerating, decelerating or stopping. Using any measures of central tendency may cause the number to cancel itself out, between the stages of accelerating and decelerating.

Shown below is a simple illustration of how speed fluctuates during a trip. For the purpose of simulation, I have used a sine curve.

```{r}
X <- seq(0, 10, 0.01)
Y <- sin(X) + 2

plot(X, Y, xlab = "Time", ylab = "Speed", type = "l", lwd = 2)
points(pi / 2, sin(pi / 2) + 2, pch = 19)
points(pi / 2, sin(pi / 2) + 2 - 0.1, pch = "A")
points(pi * 1.5, sin(pi * 1.5) + 2, pch = 19)
points(pi * 1.5, sin(pi * 1.5) + 2 + 0.1, pch = "B")
points(pi * 2.5, sin(pi * 2.5) + 2, pch = 19)
points(pi * 2.5, sin(pi * 2.5) + 2 - 0.1, pch = "C")

X2 <- seq(pi * 0.5 - 0.5, pi * 0.5 + 0.5, 0.01)
Y2 <- sin(X2) + 2

lines(X2, Y2, lwd = 2, col = "red")

X3 <- seq(pi * 1.5 - 0.5, pi * 1.5 + 0.5, 0.01)
Y3 <- sin(X3) + 2

lines(X3, Y3, lwd = 2, col = "blue")




```

From the plot above, point A is a local maximum, where the driver reaches a maximum speed, compared to the instances right before and after that point. Whereas point B is the local minimum.

The interval from point A to point B is a decelerating phase, while the interval from point B to point C is an accelerating phase.

From a local maximum point such as point A, the speed right before and after it will be very informative. This is shown by the red line. It helps us answer questions such as, did the driver accelerate excessively before suddenly braking? Similarly, information around point B (blue line) helps us understand if the driver suddenly decelerated the car, reaching the local minimum, then speed up in a short matter of time?

One special case of local minima is when the car is completely stopped, i.e. when speed is at 0. This usually happens when the car is at a red light or a busy junction. Again, we would like to know how fast the car decelerated right before it came to a complete stop, and how fast the car accelerated once it started moving again.

The phases of accelerating and decelerating allow us to aggregate the fields more easily without the worry that the numbers may cancel each other out.

### Defining the Phases

While things may look simple based on the plot above, reality is hardly so. We do not have readily available the data which tells us if a certain point is a local maximum or local minimum.

Therefore, we have to define such phases from the data given.

First of all, for any point of the data, I have used the lag and lead speed for up to three seconds. From there, I define the phases as such:

Stop:

1. Speed equals to 0; OR
2. Speed less than 0.2 AND Speed (Lag 1) equals to 0 AND Speed (Lead 1) equals to 0; OR
3. Speed less than 0.2 AND Speed (Lag 2) equals to 0 AND Speed (Lead 2) equals to 0; OR
4. Speed less than 0.2 AND Speed (Lag 3) equals to 0 AND Speed (Lead 3) equals to 0

Stops are defined as such because in the data, there are several points between stops where the car has picked up a very small amount of speed (around 0.1 to 0.2). For the purpose of practicality, those points should still be considered stops.

Local Minimum:

1. Speed less than Speed (Lag 1 to 3) and Speed (Lead 1 to 3); OR
2. Speed less than Speed (Lag 1 to 3) and Speed (Lead 1 to 3) are not available; OR
2. Speed less than Speed (Lead 1 to 3) and Speed (Lag 1 to 3) are not available






[Lead 1 through 3, Lag 1 through 3, Local Min, Local Max, Stop]

## Some Visualizations

# Model Training

Based on the features, I have trained a model using the XGB algorithm.

## Parameter Tuning

In order to get the optimal results, some parameters for the algorithm have to be tuned. This is done by using cross-validation.

As a Grid Search will be very time and memory consuming, I have opted to tune the parameters one at a time, while keeping the other parameters at their default values.

### Learning Parameters

The parameters which I have tuned are:

eta: [What is eta?]

maximum_depth

subsample

colsaple_by_tree

As we can see from the plots above, even when using the same algorithm, the parameters can have a significant effect on the model training.

### Number of Features


### Number of Rounds

Another obvious thing is that after a certain number of iterations, the performance may actually deteriorate. Therefore, the number of iterations in model training has to be tuned as well. [after having the parameters tuned] The results are shown in the plot below.

[plot for iteration]


# Results

[AUC Plot, Figure]
[How to use the model?]

