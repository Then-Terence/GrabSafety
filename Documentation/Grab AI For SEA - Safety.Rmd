---
title: "Grab AI For SEA - Safety"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(error = recover)
library(knitr)
library(xtable)
library(data.table)
```

# Grab AI For SEA - Safety

This repository is created for the submission for the Grab AI For SEA Challenge (https://www.aiforsea.com/).

As part of the challenge, I have signed up for the question under the theme of Safety. I have used the R programming language in this challenge.

The scripts used in this project, including the one used for generating this document can be found in the folder "Working Code". This document is generated by the R Markdown file "Working Code/Output/Documentation/Grab AI For SEA.Rmd".

# The Data

The data provided is made up of:
1. Telematics Data
2. Label of Dangerous Driving

The telematics data has a total number of 16135561 rows and 11 columns, split into 10 csv files.

The following is the description as obtained from the website of the competition (www.aiforsea.com/safety)

```{r, results = "asis"}
FieldDesc <-
  data.table(Field = c("bookingID", "Accuracy", "Bearing", "acceleration_x",
                       "acceleration_y", "acceleration_z", "gyro_x", "gyro_y",
                       "gyro_z", "second", "Speed"),
             Description = c("trip id", "accuracy inferred by GPS in meters",
                             "GPS bearing in degree",
                             "accelerometer reading at x axis (m/s2)",
                             "accelerometer reading at y axis (m/s2)",
                             "accelerometer reading at z axis (m/s2)",
                             "gyroscrope reading in x axis (rad/s)",
                             "gyroscope reading in y axis (rad/s)",
                             "gyroscope reading in z axis (rad/s)",
                             "time of the record by number of seconds",
                             "speed measured by GPS in m/s"))

FieldDesc <- kable(FieldDesc)
print(FieldDesc)

```

The label of dangerous driving records in binary form of whether a trip is considered dangerous, 1 if yes and 0 otherwise.

Note that there are some of the bookings with multiple labels. In such instances, I have classified the particular booking as dangerous driving if there is at least ONE label of 1.

# Feature Engineering

Based on the fields in the telematics data, feature engineering has to be conducted in order to obtain aggregate features that are meaningful in determining the riskiness in a particular booking.

## Key Concepts Applied

At the first sight, while it is tempting to just aggregate the numbers in the telematics data as they are, it can be highly unintuitive.

The reason is because the car may be accelerating, decelerating or stopping. Using any measures of central tendency may cause the number to cancel itself out, between the stages of accelerating and decelerating.

Shown below is a simple illustration of how speed fluctuates during a trip. For the purpose of simulation, I have used a sine curve.

```{r}
X <- seq(0, 10, 0.01)
Y <- sin(X) + 2

plot(X, Y, xlab = "Time", ylab = "Speed", type = "l", lwd = 2)
points(pi / 2, sin(pi / 2) + 2, pch = 19)
points(pi / 2, sin(pi / 2) + 2 - 0.1, pch = "A")
points(pi * 1.5, sin(pi * 1.5) + 2, pch = 19)
points(pi * 1.5, sin(pi * 1.5) + 2 + 0.1, pch = "B")

X2 <- seq(pi * 0.5 - 0.5, pi * 0.5 + 0.5, 0.01)
Y2 <- sin(X2) + 2

lines(X2, Y2, lwd = 2, col = "red")

X3 <- seq(pi * 1.5 - 0.5, pi * 1.5 + 0.5, 0.01)
Y3 <- sin(X3) + 2

lines(X3, Y3, lwd = 2, col = "blue")


```

From the plot above, point A is a local maximum, where the driver reaches a maximum speed, compared to the instances right before and after that point. Whereas point B is the local minimum.

From a local maximum point such as point A, the speed right before and after it will be very informative. It helps us answer questions such as, did the driver accelerate excessively before suddenly braking? Similar, information around point B helps us understand if the driver suddenly decelerated the car, reaching the local minimum, then speed up in a short matter of time?




[Lead 1 through 3, Lag 1 through 3, Local Min, Local Max, Stop]

## Some Visualizations

# Model Training

Based on the features, I have trained a model using the XGB algorithm.

## Parameter Tuning

In order to get the optimal results, some parameters for the algorithm have to be tuned. This is done by using cross-validation.

As a Grid Search will be very time and memory consuming, I have opted to tune the parameters one at a time, while keeping the other parameters at their default values.

### Learning Parameters

The parameters which I have tuned are:

eta: [What is eta?]

maximum_depth

subsample

colsaple_by_tree

As we can see from the plots above, even when using the same algorithm, the parameters can have a significant effect on the model training.

### Number of Features


### Number of Rounds

Another obvious thing is that after a certain number of iterations, the performance may actually deteriorate. Therefore, the number of iterations in model training has to be tuned as well. [after having the parameters tuned] The results are shown in the plot below.

[plot for iteration]


# Results

[AUC Plot, Figure]
[How to use the model?]

