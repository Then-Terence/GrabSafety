---
title: "Grab AI For SEA - Safety"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(error = recover)
library(knitr)
library(ggplot2)
library(data.table)
```

# Grab AI For SEA - Safety

This repository is created for the submission for the Grab AI For SEA Challenge (https://www.aiforsea.com/).

As part of the challenge, I have signed up for the question under the theme of Safety. I have used the R programming language in this challenge.

The scripts used in this project, including the one used for generating this document can be found in the folder "Working Code". This document is generated by the R Markdown file "Working Code/Output/Documentation/Grab AI For SEA.Rmd".

# The Data

The data provided is made up of:
1. Telematics Data
2. Label of Dangerous Driving

The telematics data has a total number of 16135561 rows and 11 columns, split into 10 csv files.

The following is the description as obtained from the website of the competition (www.aiforsea.com/safety)

```{r, results = "asis"}
FieldDesc <-
  data.table(Field = c("bookingID", "Accuracy", "Bearing", "acceleration_x",
                       "acceleration_y", "acceleration_z", "gyro_x", "gyro_y",
                       "gyro_z", "second", "Speed"),
             Description = c("trip id", "accuracy inferred by GPS in meters",
                             "GPS bearing in degree",
                             "accelerometer reading at x axis (m/s2)",
                             "accelerometer reading at y axis (m/s2)",
                             "accelerometer reading at z axis (m/s2)",
                             "gyroscrope reading in x axis (rad/s)",
                             "gyroscope reading in y axis (rad/s)",
                             "gyroscope reading in z axis (rad/s)",
                             "time of the record by number of seconds",
                             "speed measured by GPS in m/s"))

FieldDesc <- kable(FieldDesc)
print(FieldDesc)

```

The label of dangerous driving records in binary form of whether a trip is considered dangerous, 1 if yes and 0 otherwise.

Note that there are some of the bookings with multiple labels. In such instances, I have classified the particular booking as dangerous driving if there is at least ONE label of 1.

# Feature Engineering

Based on the fields in the telematics data, feature engineering has to be conducted in order to obtain aggregate features that are meaningful in determining the riskiness in a particular booking.

## Key Concepts Applied

At the first sight, while it is tempting to just aggregate the numbers in the telematics data as they are, it can be highly unintuitive.

The reason is because the car may be accelerating, decelerating or stopping. Using any measures of central tendency may cause the number to cancel itself out, between the stages of accelerating and decelerating.

Shown below is a simple illustration of how speed fluctuates during a trip. For the purpose of simulation, I have used a sine curve.

```{r}
X <- seq(0, 10, 0.01)
Y <- sin(X) + 2

plot(X, Y, xlab = "Time", ylab = "Speed", type = "l", lwd = 2)
points(pi / 2, sin(pi / 2) + 2, pch = 19)
points(pi / 2, sin(pi / 2) + 2 - 0.1, pch = "A")
points(pi * 1.5, sin(pi * 1.5) + 2, pch = 19)
points(pi * 1.5, sin(pi * 1.5) + 2 + 0.1, pch = "B")
points(pi * 2.5, sin(pi * 2.5) + 2, pch = 19)
points(pi * 2.5, sin(pi * 2.5) + 2 - 0.1, pch = "C")

X2 <- seq(pi * 0.5 - 0.5, pi * 0.5 + 0.5, 0.01)
Y2 <- sin(X2) + 2

lines(X2, Y2, lwd = 2, col = "red")

X3 <- seq(pi * 1.5 - 0.5, pi * 1.5 + 0.5, 0.01)
Y3 <- sin(X3) + 2

lines(X3, Y3, lwd = 2, col = "blue")




```

From the plot above, point A is a local maximum, where the driver reaches a maximum speed, compared to the instances right before and after that point. Whereas point B is the local minimum.

The interval from point A to point B is a decelerating phase, while the interval from point B to point C is an accelerating phase.

From a local maximum point such as point A, the speed right before and after it will be very informative. This is shown by the red line. It helps us answer questions such as, did the driver accelerate excessively before suddenly braking? Similarly, information around point B (blue line) helps us understand if the driver suddenly decelerated the car, reaching the local minimum, then speed up in a short matter of time?

One special case of local minima is when the car is completely stopped, i.e. when speed is at 0. This usually happens when the car is at a red light or a busy junction. Again, we would like to know how fast the car decelerated right before it came to a complete stop, and how fast the car accelerated once it started moving again.

The phases of accelerating and decelerating allow us to aggregate the fields more easily without the worry that the numbers may cancel each other out.

### Defining the Phases

While things may look simple based on the plot above, reality is hardly so. We do not have readily available the data which tells us if a certain point is a local maximum or local minimum.

Therefore, we have to define such phases from the data given.

First of all, for any point of the data, I have used the lag and lead speed for up to three seconds. From there, I define the phases as such:

Stop:

1. Speed equals to 0; OR
2. Speed less than 0.2 AND Speed (Lag 1) equals to 0 AND Speed (Lead 1) equals to 0; OR
3. Speed less than 0.2 AND Speed (Lag 2) equals to 0 AND Speed (Lead 2) equals to 0; OR
4. Speed less than 0.2 AND Speed (Lag 3) equals to 0 AND Speed (Lead 3) equals to 0

Stops are defined as such because in the data, there are several points between stops where the car has picked up a very small amount of speed (around 0.1 to 0.2). For the purpose of practicality, those points should still be considered stops.

After defining stops, we can define the points where the stops start or end.

Start of Stop:

1. Having the status "Stop" for the current second but not the previous second.

End of Stop:

1. Having the status "Stop" for the current second but not the next second.

Local Minimum:

1. Speed less than Speed (Lag 1 to 3) and Speed (Lead 1 to 3); OR
2. Speed less than Speed (Lag 1 to 3) and Speed (Lead 1 to 3) are not available; OR
2. Speed less than Speed (Lead 1 to 3) and Speed (Lag 1 to 3) are not available

Local Maximum:

1. Speed greater than Speed (Lag 1 to 3) and Speed (Lead 1 to 3); OR
2. Speed greater than Speed (Lag 1 to 3) and Speed (Lead 1 to 3) are not available; OR
2. Speed greater than Speed (Lead 1 to 3) and Speed (Lag 1 to 3) are not available

Accelerating:

1. Speed greater than Speed (Lag 1 to 3) AND Speed less than Speed (Lead 1 to 3)

Decelerating:

1. Speed less than Speed (Lag 1 to 3) AND Speed greater than Speed (Lead 1 to 3)

Other than that, I have discovered some possible anomalies within the data, where at certain points the Speed are shown to be exactly -1m/s, and there do not seem to be any smooth transition to and from that speed a few seconds before and after. This is highly unusual.

Among those data points with Speed equals to -1m/s, a very high amount has the Bearing reading of exactly 0 as well, which seems odd and could contain some information not immediately apparent.

As such, I have also defined the following:

Anomaly1: Speed equals to -1
Anomaly2: Speed equals to -1 AND Bearing equals to 0

Data points with tags of Anomaly1 and Anomaly2 are not used in computing the features.

## Phase-based Features

Based on the six phases defined in the previous section, some features can be engineered.

### Local Minimum, Local Maximum

For the phases of Local Minimum and Local Maximum, I have aggregated the Speed from 3 seconds before to 3 seconds after by:

1. Mean
2. Standard Deviation
3. Maximum

This results in 42 features (2 phases * 7 seconds * 3 aggregations).

### Start of Stop

For the phase of Start of Stop, I have aggregated the Speed from 3 seconds before by:

1. Mean
2. Standard Deviation
3. Maximum

This results in 9 features (1 phase * 3 seconds * 3 aggregations).

### End of Stop

For the phase of End of Stop, I have aggregated the Speed from 3 seconds after by:

1. Mean
2. Standard Deviation
3. Maximum

This results in 9 features (1 phase * 3 seconds * 3 aggregations).

### Accelerating, Decelerating

For the phases of Accelerating and Decelerating, there are more data fields that will give meaningful insights, especially since there are three axes for acceleration and gyrometer readings. Some of the axes can be combined into a new field. For the purpose of brevity, I have used "acc" in place of "acceleration".

We have computed the following new fields:

acc_xy = sqrt((acc_x) ^ 2 + (acc_y) ^ 2)
acc_xyz  = sqrt((acc_x) ^ 2 + (acc_y) ^ 2 + (acc_z) ^ 2)

The acceleration for the plane xy and xyz can be computed by the formulas above, by the virtue of Pythagoras' Theorem.

The acceleration for the plane xy essentially takes into account both the acceleration to the front / back and left / right, whereas plane xyz takes into account up / down as well.

While accelerating / decelerating fast is bad enough for dangerous driving, the rate of change of acceleration or gyrometer reading will be much worse. To compute (approximate) the rate of change of acceleration and gyrometer reading, I have taken the first difference of the value.

For example:

At time t - 1, acc_x = 0.2m/s2
At time t, acc_x = 0.5m/s2

At time t, the first difference of acc_x = (0.5m/s2 - 0.2m/s2) / (1s) = 0.3m/s3

I have aggregated the following fields:

1. acc (x, y, z, xy, xyz)
2. gyro (x, y, z)

by their values and first differences, by:

1. mean
2. max
3. sd

This results in 96 features (2 phases * 8 fields * 2 (value at current second, first difference) * 3 aggregations).

There is a total of 156 phase-based features.

## Non Phase-based Features

On top of the features described above, I have computed some features not based on the phases.

Duration (computed as max(second) + 1, since it starts with 0)
No. of Local Minima
No. of Local Maxima
No. of Stops
Duration of Stops
No. of Anomaly1
No. of Anomaly2

Other than that, the finding from Accuracy is that it clusters around some common values. The 10 most common values are:

3, 3.9, 4, 5, 6, 8, 10, 12, 16, 32

When the car is at a stop, the highly common value is only 3.

I have calculated, for each bookings, the proportion of the duration that it shows such readings. The reasoning is that such values could be common by default, when the car is moving at a steady pace, that the readings only diverge from these common values when the speed / acceleration of the car is fluctuating a lot.

This results in 11 features (10 common values, Accuracy of 3 when the car is at stop).

I have also calculated the first difference of the Accuracy, and aggregate by:

1. Mean
2. Standard Deviation
3. Max

All in all, there are 21 non phase-based features.

All the 167 features can be found in the file [...], along with their names and descriptions.

# Model Training

Based on the features, I have trained a model using the XGBoost algorithm.

In order to get the optimal results, some parameters for the algorithm have to be tuned. This is done by using cross-validation.

I have used the data split of 90% for training, and 10% for testing.

## Parameter Tuning

As a Grid Search will be very time and memory consuming, I have opted to tune the parameters one at a time, while keeping the other parameters at their default values.

### Learning Parameters

The learning parameters for XGBoost which I have tuned are:

eta: [What is eta?]

maximum_depth

subsample

colsaple_by_tree

```{r, results = "asis"}
load("Output/TuneXGB.RData")
ggplot(data = test_eta) +
  geom_line(aes(x = iteration, y = value, color = variable))
ggplot(data = test_md) +
  geom_line(aes(x = iteration, y = value, color = variable))
ggplot(data = test_ss) +
  geom_line(aes(x = iteration, y = value, color = variable))
ggplot(data = test_cs) +
  geom_line(aes(x = iteration, y = value, color = variable))

```

As we can see from the plots above, even when using the same algorithm, the parameters can have a significant effect on the model training.

### Number of Features


### Number of Rounds

Another obvious thing is that after a certain number of iterations, the performance may actually deteriorate. Therefore, the number of iterations in model training has to be tuned as well. [after having the parameters tuned] The results are shown in the plot below.

[plot for iteration]


# Results

[AUC Plot, Figure]
[How to use the model?]

